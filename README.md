# SSL-pretraining-separation
This is the official repository of [SELF-SUPERVISED PRE-TRAINING REDUCES LABEL PERMUTATION INSTABILITY OF SPEECH SEPARATION
](https://arxiv.org/pdf/2010.15366.pdf), which is not organized yet. Although the code cannot be run directly, it can show the detailed experimental settings not described in the paper.

## Organizing Progress
### Corpus Preprocessing
* [x] WSJ0-2mix
* [x] Libri2Mix

## Reference
The corpus preprocessing codes were adapted from [asteroid/egs/librimix/ConvTasNet/](https://github.com/asteroid-team/asteroid/tree/master/egs/librimix/ConvTasNet) and [asteroid/egs/wham/ConvTasNet/](https://github.com/asteroid-team/asteroid/tree/master/egs/wham/ConvTasNet).
